{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**AI Study Assistant **\n",
        "*This project helps you to *\n",
        "\n",
        "\n",
        "1. -> Summarize the content\n",
        "2. -> Question about the content\n",
        "3. -> Question and Answer on different Topics\n",
        "4. -> Generate Thesis Article Blog post Essays\n",
        "5. -> upload pdf file to summarize the content\n"
      ],
      "metadata": {
        "id": "GpDkyuZzExfQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "KEYoPYCnDp6I"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -qU langchain\n",
        "\n",
        "# Securely store the API key in Google Colab secrets manager\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Import the required libraries for Google Generative AI and LangChain\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "\n",
        "# Configure the generative AI client\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y9ShuHObF6zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*   Summarize you content\n",
        "\n",
        "*   Questin about the content\n",
        "\n"
      ],
      "metadata": {
        "id": "3K5_QscdFv3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summarization prompt template\n",
        "summarization_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Input variable to pass text for summarization\n",
        "    template=\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\",\n",
        ")\n",
        "\n",
        "# Initialize the LangChain Gemini LLM with gemini-1.5-flash model\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Create the summarization chain\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
        "\n",
        "# Get user input for the topic and sample text\n",
        "sample_text_input = input(\"Enter the sample text for summarization: \")\n",
        "\n",
        "# Run the summarization chain with user input\n",
        "try:\n",
        "    summary = llm.invoke([HumanMessage(content=summarization_prompt.format(text=sample_text_input))]).content\n",
        "    print(\"Summary:\")\n",
        "    display(Markdown(summary))\n",
        "\n",
        "    # Optionally, you can extend this to add more functionality such as Q&A based on the summarized text or topic\n",
        "    # For example, a Q&A prompt template can be added as follows:\n",
        "    qa_prompt = PromptTemplate(\n",
        "        input_variables=[\"text\", \"question\"],\n",
        "        template=\"Answer the following question based on the text: \\n{text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "    )\n",
        "\n",
        "    qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
        "\n",
        "    # Get user input for the question\n",
        "    question_input = input(f\"Enter a question related to the topic': \")\n",
        "\n",
        "    # Run the Q&A chain\n",
        "    answer = qa_chain.run({\"text\": sample_text_input, \"question\": question_input})\n",
        "    print(f\"Answer to your question: {answer}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "2L7un5m-EWe6",
        "outputId": "09606b26-df91-48fd-d580-cbee8eaf9ac3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary:\n"
          ]
        },
        {
          "data": {
            "text/markdown": "Despite facing criminal investigations, a civil judgment for sexual abuse, and an attempt by his supporters to overturn the 2020 election, the former US president has staged a remarkable political comeback.  He's now a significant influence within the Republican party, questioning continued support for Ukraine and even hosting foreign leaders, making a return to power a real possibility.  His enduring polarizing nature remains a key aspect of his political presence.\n",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a question related to the topic': will ukrain get free from russia\n",
            "Answer to your question: The provided text does not offer a prediction on whether Ukraine will gain freedom from Russia.  It only mentions that a particular individual (implied to be Donald Trump) has cast doubt on continued support for Ukraine's fight against the Russian invasion.  Therefore, the answer is **unknown**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Ask Questions on various Topics\n",
        "\n"
      ],
      "metadata": {
        "id": "wNSKpgk-GFqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\", \"question\"],  # Inputs: text and question\n",
        "    template=\"Answer the following question based on the text: \\n{text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "I-QRQkQFQzRw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n"
      ],
      "metadata": {
        "id": "ggliD0a5Q2UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_chain = LLMChain(llm=llm, prompt=generation_prompt)\n",
        "\n",
        "topic_input = input(\"Enter the topic to generate the text for = \")\n",
        "generated_text = generation_chain.run({\"topic\": topic_input})\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJJ8I3S-RA60",
        "outputId": "f054a73b-5b5f-4e29-c96d-4468ed4887be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter the topic to generate the text forhow ai going to change the future of kids\n",
            "## AI: Shaping the Future for a Generation of Kids\n",
            "\n",
            "Artificial intelligence (AI) is no longer a futuristic fantasy; it's rapidly weaving itself into the fabric of our daily lives, and its impact on children will be profound and multifaceted.  From personalized learning experiences to ethical dilemmas surrounding its use, AI's influence on the future generation is a complex topic demanding careful consideration.\n",
            "\n",
            "**1. Revolutionizing Education:**\n",
            "\n",
            "AI's potential to personalize education is arguably its most transformative aspect for kids.  Imagine a learning system that adapts to each child's individual pace, learning style, and strengths and weaknesses.  This is becoming a reality through:\n",
            "\n",
            "* **Intelligent Tutoring Systems:** AI-powered platforms can provide customized feedback, identify knowledge gaps, and offer targeted exercises, ensuring every child receives the support they need. This can be particularly beneficial for students with learning disabilities or those who require extra attention.\n",
            "* **Adaptive Assessments:**  AI can create dynamic assessments that adjust difficulty based on a child's performance, providing a more accurate measure of their understanding and identifying areas requiring further focus.\n",
            "* **Personalized Learning Paths:** AI can analyze a child's learning progress and suggest tailored learning materials, activities, and resources, optimizing their learning journey.\n",
            "* **Automated Grading and Feedback:** This frees up teachers' time, allowing them to focus on individualized instruction and student interaction.\n",
            "\n",
            "\n",
            "**2. Transforming Play and Entertainment:**\n",
            "\n",
            "AI is also reshaping how children play and interact with entertainment:\n",
            "\n",
            "* **AI-Powered Toys:**  Interactive toys that respond to children's actions and learn their preferences are becoming increasingly common.  These toys can promote creativity, problem-solving skills, and social-emotional development.\n",
            "* **Personalized Content Recommendations:**  AI algorithms curate age-appropriate content, ensuring children have access to engaging and educational materials. However, this also raises concerns about filter bubbles and limited exposure to diverse perspectives.\n",
            "* **Immersive Gaming Experiences:** AI is enhancing gaming experiences, making them more realistic, engaging, and adaptive to player skills.  This can foster creativity, collaboration, and problem-solving skills, but also raises concerns about excessive screen time and potential addiction.\n",
            "\n",
            "\n",
            "**3.  Impact on Social and Emotional Development:**\n",
            "\n",
            "AI's influence extends beyond academics and entertainment; it will impact children's social and emotional development:\n",
            "\n",
            "* **Social Robots:** Robots designed to interact with children can provide companionship, emotional support, and even teach social skills. However, the ethical implications of relying on robots for emotional support need careful consideration.\n",
            "* **AI-driven Mental Health Support:** AI-powered tools can help identify and address mental health challenges in children, providing early intervention and support.  However, maintaining privacy and ensuring ethical data handling are crucial.\n",
            "* **Potential for Social Isolation:**  Excessive reliance on AI-driven technologies could lead to social isolation and hinder the development of crucial interpersonal skills.\n",
            "\n",
            "\n",
            "**4.  Ethical Considerations and Challenges:**\n",
            "\n",
            "The rapid advancement of AI presents several ethical challenges:\n",
            "\n",
            "* **Data Privacy and Security:**  Protecting children's data is paramount.  Regulations and safeguards are needed to prevent misuse and ensure responsible data handling.\n",
            "* **Algorithmic Bias:** AI algorithms can perpetuate existing biases, leading to unfair or discriminatory outcomes.  Addressing bias in AI systems is crucial for ensuring equity and fairness.\n",
            "* **Job Displacement:**  Automation driven by AI could lead to job displacement in various sectors, impacting future employment opportunities for children.\n",
            "* **Over-Reliance and Dependence:**  Excessive dependence on AI could hinder the development of critical thinking, problem-solving, and creativity.\n",
            "\n",
            "\n",
            "**5.  Preparing Children for the AI-Driven Future:**\n",
            "\n",
            "To prepare children for an AI-driven future, we need:\n",
            "\n",
            "* **AI Literacy Education:**  Integrating AI literacy into the curriculum is essential to equip children with the knowledge and skills to understand and interact with AI systems responsibly.\n",
            "* **Critical Thinking and Problem-Solving Skills:**  Developing these skills is crucial to navigate the complexities of an AI-driven world.\n",
            "* **Ethical Awareness:**  Educating children about the ethical implications of AI is crucial to promote responsible use and prevent harm.\n",
            "\n",
            "\n",
            "In conclusion, AI's impact on the future of children will be transformative.  While it offers tremendous potential to enhance learning, entertainment, and well-being, it also presents significant challenges that demand careful consideration and proactive measures.  By fostering AI literacy, promoting ethical awareness, and addressing potential risks, we can harness the power of AI to create a brighter and more equitable future for the next generation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a summarization prompt template\n",
        "summarization_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],  # Input variable to pass text for summarization\n",
        "    template=\"Summarize the following text:\\n\\n{text}\\n\\nSummary:\",\n",
        ")\n",
        "\n",
        "# Initialize the LangChain Gemini LLM with gemini-1.5-flash model\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Create the summarization chain\n",
        "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)\n",
        "\n",
        "# Function to read text from uploaded file\n",
        "def read_uploaded_file(uploaded_file):\n",
        "    file_name = uploaded_file.name\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        # Read a .txt file\n",
        "        return uploaded_file.read().decode(\"utf-8\")\n",
        "    elif file_name.endswith(\".pdf\"):\n",
        "        # Read a .pdf file using PyPDF2\n",
        "        pdf_reader = PyPDF2.PdfReader(uploaded_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    else:\n",
        "        return \"Unsupported file type.\"\n",
        "\n",
        "# Ask the user to upload a document\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming the first file uploaded is the one to process\n",
        "uploaded_file = list(uploaded.values())[0]\n",
        "\n",
        "# Read the uploaded document's content\n",
        "file_content = read_uploaded_file(uploaded_file)\n",
        "\n",
        "# Check if the file content is not empty\n",
        "if file_content.strip():\n",
        "    try:\n",
        "        # Run the summarization chain with the content of the uploaded document\n",
        "        summary = llm.invoke([HumanMessage(content=summarization_prompt.format(text=file_content))]).content\n",
        "        print(\"Summary of the uploaded document:\")\n",
        "        display(Markdown(summary))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"No valid content found in the uploaded document.\")\n"
      ],
      "metadata": {
        "id": "FXW5N_2MeXTC",
        "outputId": "50ae4b34-f702-49c0-ad52-1f133bae4942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-69731d1a-f50f-42ff-b98b-0f1da3d94bcc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-69731d1a-f50f-42ff-b98b-0f1da3d94bcc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install -qU langchain-google-genai\n",
        "!pip install -qU langchain\n",
        "!pip install -q PyPDF2  # For reading PDF files\n",
        "\n",
        "# Securely store the API key in Google Colab secrets manager\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Import the required libraries for Google Generative AI and LangChain\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "from google.colab import files\n",
        "import PyPDF2\n",
        "\n",
        "# Configure the generative AI client\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Initialize the LangChain Gemini LLM with gemini-1.5-flash model\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-flash\", api_key=GEMINI_API_KEY)\n",
        "\n",
        "# Create a prompt template for content generation based on topic and content type\n",
        "generation_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\", \"content_type\"],  # Input variables for topic and content type\n",
        "    template=\"Generate a {content_type} about the following topic: {topic}\"\n",
        ")\n",
        "\n",
        "# Create the generation chain\n",
        "generation_chain = LLMChain(llm=llm, prompt=generation_prompt)\n",
        "\n",
        "# Function to read text from uploaded file\n",
        "def read_uploaded_file(uploaded_file):\n",
        "    file_name = uploaded_file.name\n",
        "    if file_name.endswith(\".txt\"):\n",
        "        # Read a .txt file\n",
        "        return uploaded_file.read().decode(\"utf-8\")\n",
        "    elif file_name.endswith(\".pdf\"):\n",
        "        # Read a .pdf file using PyPDF2\n",
        "        pdf_reader = PyPDF2.PdfReader(uploaded_file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    else:\n",
        "        return \"Unsupported file type.\"\n",
        "\n",
        "# Ask the user to upload a document (optional)\n",
        "upload_option = input(\"Would you like to upload a document to summarize? (yes/no): \").lower()\n",
        "\n",
        "if upload_option == \"yes\":\n",
        "    uploaded = files.upload()\n",
        "    uploaded_file = list(uploaded.values())[0]\n",
        "    file_content = read_uploaded_file(uploaded_file)\n",
        "else:\n",
        "    file_content = \"\"\n",
        "\n",
        "# Ask the user for the topic\n",
        "topic_input = input(\"Enter the topic for generating content: \")\n",
        "\n",
        "# Ask the user about the type of content they want to generate\n",
        "content_type_input = input(\"What type of content would you like to generate? (assignemnts/thesis/summary/essay/blogpost/article): \").lower()\n",
        "\n",
        "# If the user provided a document, summarize its content, otherwise generate content based on topic and type\n",
        "if file_content.strip():\n",
        "    try:\n",
        "        # Run the summarization chain with the content of the uploaded document\n",
        "        summary = llm.invoke([HumanMessage(content=\"Summarize the following text:\\n\\n\" + file_content)]).content\n",
        "        print(\"Summary of the uploaded document:\")\n",
        "        display(Markdown(summary))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    try:\n",
        "        # Run the content generation chain with the topic and content type\n",
        "        generated_text = generation_chain.run({\"topic\": topic_input, \"content_type\": content_type_input})\n",
        "        print(f\"Generated {content_type_input.capitalize()} about {topic_input}:\")\n",
        "        display(Markdown(generated_text))\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n"
      ],
      "metadata": {
        "id": "U2UhHLuTsgMA",
        "outputId": "acf3b843-c423-4b39-eb44-1a3f92d6a72a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Would you like to upload a document to summarize? (yes/no): no\n",
            "Enter the topic for generating content: explore how os like kubernetes manage cloude resources\n",
            "What type of content would you like to generate? (assignemnts/thesis/summary/essay/blogpost/article): article\n",
            "Generated Article about explore how os like kubernetes manage cloude resources:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## Orchestrating the Cloud: How Kubernetes Manages Cloud Resources\n\nCloud computing offers unparalleled scalability and flexibility, but managing the vast array of resources – virtual machines, containers, storage, networking – can quickly become overwhelming. This is where container orchestration systems like Kubernetes step in, acting as sophisticated conductors of the cloud orchestra.  Kubernetes doesn't directly manage *all* cloud resources (it doesn't, for example, directly manage the underlying physical hardware), but it expertly handles the deployment, scaling, and management of applications running within containers across a cluster of machines.\n\n**Abstraction and Resource Management:**\n\nKubernetes's core strength lies in its ability to abstract away the complexities of the underlying cloud infrastructure.  Instead of dealing with individual virtual machines or physical servers, developers interact with Kubernetes through declarative configurations.  They specify *what* they want – the desired state of their application – and Kubernetes figures out *how* to achieve it.\n\nThis involves intelligent resource allocation.  Kubernetes monitors available CPU, memory, storage, and network resources across the cluster.  Based on the resource requests defined in application deployments, it schedules containers onto nodes (virtual machines or physical servers) where sufficient resources are available.  This dynamic allocation ensures efficient utilization of cloud resources and prevents resource contention.\n\n**Scaling and Auto-Scaling:**\n\nOne of the most significant benefits of Kubernetes is its inherent scalability.  Scaling applications up or down is a simple matter of adjusting the desired number of replicas in a deployment configuration.  Kubernetes automatically handles the creation or deletion of pods (the smallest deployable units in Kubernetes) to match the specified replica count.\n\nFurthermore, Kubernetes integrates seamlessly with cloud provider's auto-scaling features.  Based on metrics like CPU utilization or request rate, Kubernetes can automatically scale applications up or down, ensuring optimal performance and cost efficiency.  This eliminates the need for manual intervention and allows applications to adapt dynamically to changing demands.\n\n**Networking and Service Discovery:**\n\nKubernetes manages the networking layer within the cluster, providing services with stable internal and external IP addresses.  This simplifies communication between containers and enables applications to be easily accessed from outside the cluster.  Kubernetes's service discovery mechanism ensures that containers can find and communicate with each other regardless of their location within the cluster.  This is crucial for microservices architectures, where applications are composed of many interconnected services.\n\n**Storage Management:**\n\nKubernetes also provides mechanisms for managing persistent storage.  This allows applications to retain data even if pods are restarted or rescheduled.  Kubernetes supports various storage solutions, including cloud-native storage services offered by cloud providers like Amazon EBS, Google Persistent Disk, and Azure Disk.  This allows developers to choose the storage solution that best suits their application's needs.\n\n**Integration with Cloud Providers:**\n\nKubernetes works seamlessly with major cloud providers like AWS, Azure, and Google Cloud Platform (GCP).  These providers offer managed Kubernetes services (like EKS, AKS, and GKE) that simplify the deployment and management of Kubernetes clusters.  These managed services handle many of the underlying infrastructure concerns, allowing developers to focus on their applications.\n\n**Conclusion:**\n\nKubernetes is a powerful tool for managing cloud resources, providing a robust and scalable platform for deploying and managing containerized applications.  Its abstraction layer, intelligent resource allocation, auto-scaling capabilities, and seamless integration with cloud providers make it an essential component of modern cloud-native architectures.  By automating many of the complex tasks involved in managing cloud resources, Kubernetes empowers developers to build and deploy applications faster, more efficiently, and with greater scalability.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}